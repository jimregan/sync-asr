{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.diarize import DiarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HF_DIR = Path.home() / \".huggingface\"\n",
    "HF_TOKEN = HF_DIR / \"token\"\n",
    "TOKEN = \"\"\n",
    "if HF_DIR.is_dir() and HF_TOKEN.exists():\n",
    "    with open(str(HF_TOKEN)) as hf_tok:\n",
    "        TOKEN = hf_tok.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_pipe = DiarizationPipeline(use_auth_token=TOKEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = Path(\"/home/joregan/hsi/audio\")\n",
    "EG = AUDIO_PATH / \"hsi_3_0715_227_001_inter-002.wav\"\n",
    "diar_res = diar_pipe(str(EG), num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, diar_seg in diar_res.iterrows():\n",
    "    print(diar_seg[\"start\"], diar_seg[\"end\"], diar_seg[\"speaker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is just to confirm that the output of `merge_chunks` is similar in terms of timestamps to the diarisation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from whisperx.vad import load_vad_model, merge_chunks\n",
    "from whisperx.audio import load_audio, SAMPLE_RATE\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L336\n",
    "default_vad_options = {\n",
    "    \"vad_onset\": 0.500,\n",
    "    \"vad_offset\": 0.363\n",
    "}\n",
    "\n",
    "audio = load_audio(str(EG))\n",
    "\n",
    "chunk_size = 30\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L186\n",
    "vad_model = load_vad_model(torch.device(DEVICE), use_auth_token=None, **default_vad_options)\n",
    "vad_segments = vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "vad_segments = merge_chunks(\n",
    "    vad_segments,\n",
    "    chunk_size,\n",
    "    onset=default_vad_options[\"vad_onset\"],\n",
    "    offset=default_vad_options[\"vad_offset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.008532423208191127,\n",
       " 'end': 27.329351535836178,\n",
       " 'segments': [(0.008532423208191127, 2.841296928327645),\n",
       "  (5.6058020477815695, 10.179180887372015),\n",
       "  (11.527303754266212, 12.022184300341298),\n",
       "  (23.523890784982935, 27.329351535836178)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarised_chunks(filename):\n",
    "    diar_res = diar_pipe(filename, num_speakers=2)\n",
    "    res = []\n",
    "    for idx, diar_seg in diar_res.iterrows():\n",
    "        res.append({\n",
    "            \"start\": diar_seg[\"start\"],\n",
    "            \"end\": diar_seg[\"end\"],\n",
    "            \"segments\": [(diar_seg[\"start\"], diar_seg[\"end\"])],\n",
    "            \"speaker\": diar_seg[\"speaker\"]\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def write_wave(filename, data):\n",
    "    data_denorm = data * 32768.0\n",
    "    data16 = data_denorm.astype(np.int16)\n",
    "    output = wave.open(filename, \"w\")\n",
    "    # pcm_s16le, single channel\n",
    "    output.setnchannels(1)\n",
    "    output.setsampwidth(2)\n",
    "    output.setframerate(16000)\n",
    "    output.writeframes(data16.tobytes())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my reference\n",
    "_FORMATS = \"\"\"\n",
    "hsi_N_NNNN_NNN_NNN-mic.wav\n",
    "hsi_N_NNNN_NNN_NNN-micN-NNN.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_main.wav\n",
    "\"\"\"\n",
    "def get_speaker_id(filename):\n",
    "    if \"inter\" in filename or \"mic2\" in filename:\n",
    "        part = \"inter\"\n",
    "    elif \"main\" in filename or \"mic1\" in filename:\n",
    "        part = \"main\"\n",
    "    elif filename.endswith(\"-mic.wav\"):\n",
    "        # one file\n",
    "        part = \"inter\"\n",
    "    pieces = filename.split(\"_\")\n",
    "    return f\"hsi_{pieces[1]}_{part}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory(speaker_id, base_dir=\"/home/joregan/hsi_mfa\"):\n",
    "    base_path = Path(base_dir)\n",
    "    if not base_path.is_dir():\n",
    "        base_path.mkdir()\n",
    "    speaker_path = base_path / speaker_id\n",
    "    if not speaker_path.is_dir():\n",
    "        speaker_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from whisperx.types import TranscriptionResult\n",
    "from typing import List, Union\n",
    "import faster_whisper\n",
    "from whisperx.asr import find_numeral_symbol_tokens, SingleSegment\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L173\n",
    "def transcribe(\n",
    "    self, audio: Union[str, np.ndarray], batch_size=None, num_workers=0, language=None, task=None, chunk_size=30, print_progress = False, combined_progress=False\n",
    ") -> TranscriptionResult:\n",
    "    filename = audio\n",
    "    if isinstance(audio, str):\n",
    "        audio = load_audio(audio)\n",
    "\n",
    "    def data(audio, segments):\n",
    "        for seg in segments:\n",
    "            f1 = int(seg['start'] * SAMPLE_RATE)\n",
    "            f2 = int(seg['end'] * SAMPLE_RATE)\n",
    "            # print(f2-f1)\n",
    "            yield {'inputs': audio[f1:f2]}\n",
    "\n",
    "    # vad_segments = self.vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "    # vad_segments = merge_chunks(\n",
    "    #     vad_segments,\n",
    "    #     chunk_size,\n",
    "    #     onset=self._vad_params[\"vad_onset\"],\n",
    "    #     offset=self._vad_params[\"vad_offset\"],\n",
    "    # )\n",
    "    vad_segments = get_diarised_chunks(filename)\n",
    "    if self.tokenizer is None:\n",
    "        language = language or self.detect_language(audio)\n",
    "        task = task or \"transcribe\"\n",
    "        self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                            self.model.model.is_multilingual, task=task,\n",
    "                                                            language=language)\n",
    "    else:\n",
    "        language = language or self.tokenizer.language_code\n",
    "        task = task or self.tokenizer.task\n",
    "        if task != self.tokenizer.task or language != self.tokenizer.language_code:\n",
    "            self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                                self.model.model.is_multilingual, task=task,\n",
    "                                                                language=language)\n",
    "            \n",
    "    if self.suppress_numerals:\n",
    "        previous_suppress_tokens = self.options.suppress_tokens\n",
    "        numeral_symbol_tokens = find_numeral_symbol_tokens(self.tokenizer)\n",
    "        print(f\"Suppressing numeral and symbol tokens\")\n",
    "        new_suppressed_tokens = numeral_symbol_tokens + self.options.suppress_tokens\n",
    "        new_suppressed_tokens = list(set(new_suppressed_tokens))\n",
    "        self.options = self.options._replace(suppress_tokens=new_suppressed_tokens)\n",
    "\n",
    "    segments: List[SingleSegment] = []\n",
    "    batch_size = batch_size or self._batch_size\n",
    "    total_segments = len(vad_segments)\n",
    "    for idx, out in enumerate(self.__call__(data(audio, vad_segments), batch_size=batch_size, num_workers=num_workers)):\n",
    "        if print_progress:\n",
    "            base_progress = ((idx + 1) / total_segments) * 100\n",
    "            percent_complete = base_progress / 2 if combined_progress else base_progress\n",
    "            print(f\"Progress: {percent_complete:.2f}%...\")\n",
    "        text = out['text']\n",
    "        if batch_size in [0, 1, None]:\n",
    "            text = text[0]\n",
    "        # write_mfa()\n",
    "        segments.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"start\": round(vad_segments[idx]['start'], 3),\n",
    "                \"end\": round(vad_segments[idx]['end'], 3),\n",
    "                \"speaker\": vad_segments[idx]['speaker']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # revert the tokenizer if multilingual inference is enabled\n",
    "    if self.preset_language is None:\n",
    "        self.tokenizer = None\n",
    "\n",
    "    # revert suppressed tokens if suppress_numerals is enabled\n",
    "    if self.suppress_numerals:\n",
    "        self.options = self.options._replace(suppress_tokens=previous_suppress_tokens)\n",
    "\n",
    "    return {\"segments\": segments, \"language\": language}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "\n",
    "- [ ] write MFA files\n",
    "  - [X] write wav\n",
    "  - [ ] normalise text\n",
    "  - [ ] write text\n",
    "  - [ ] write offsets against original file\n",
    "- [ ] write basic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (0.98) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import types\n",
    "import gc\n",
    "\n",
    "compute_type = \"float16\"\n",
    "model = whisperx.load_model(\"large-v2\", DEVICE, compute_type=compute_type)\n",
    "model.transcribe = types.MethodType(transcribe, model)\n",
    "result = model.transcribe(audio=str(EG), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segments': [{'text': \" thank you for showing me or something like that and that's the end.\",\n",
       "   'start': 0.008,\n",
       "   'end': 2.81,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Because I am seeing her so maybe she can say like just wrap it up.',\n",
       "   'start': 5.611,\n",
       "   'end': 10.365,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah.', 'start': 11.52, 'end': 12.012, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' So hey, how are you? I am here. I mean, everything is gorgeous here.',\n",
       "   'start': 23.523,\n",
       "   'end': 27.292,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" I'm good, I'm good.\",\n",
       "   'start': 30.722,\n",
       "   'end': 31.689,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you', 'start': 37.767, 'end': 37.903, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Subs by www.zeoranger.co.uk',\n",
       "   'start': 38.005,\n",
       "   'end': 38.209,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': \" I love how I'm facing the future.\",\n",
       "   'start': 38.362,\n",
       "   'end': 42.47,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' So...', 'start': 43.302, 'end': 43.778, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Yeah.', 'start': 44.372, 'end': 44.915, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Yeah.', 'start': 45.102, 'end': 45.866, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' it.', 'start': 46.681, 'end': 47.003, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' I mean, it looks really comfortable. Where do you get it?',\n",
       "   'start': 49.907,\n",
       "   'end': 53.081,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' themselves.',\n",
       "   'start': 58.005,\n",
       "   'end': 58.99,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' you', 'start': 60.806, 'end': 60.959, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Oh, I see.',\n",
       "   'start': 61.74,\n",
       "   'end': 62.946,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" Wow, that's great, because it seems really good quality. So if you bought it even half price, it must be a really good deal.\",\n",
       "   'start': 65.153,\n",
       "   'end': 73.642,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' And I also really like the color.',\n",
       "   'start': 75.594,\n",
       "   'end': 76.952,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" It's really flexible, I think.\",\n",
       "   'start': 81.265,\n",
       "   'end': 83.591,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah.', 'start': 83.693, 'end': 84.321, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah.', 'start': 97.36, 'end': 97.818, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" Wow, okay. Yeah, that's great. I think though you could add a blanket because that would be really nice like for watching TV or this kind of things.\",\n",
       "   'start': 103.387,\n",
       "   'end': 112.012,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" But it's a little bit low though, so I have to get up some good ways.\",\n",
       "   'start': 120.263,\n",
       "   'end': 123.285,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Oh, I see. Yeah.',\n",
       "   'start': 124.015,\n",
       "   'end': 125.832,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' I see you have a really nice view, like in the balcony.',\n",
       "   'start': 129.516,\n",
       "   'end': 132.521,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" You're really close to nature, that's really cool. You can make a picnic or something, that would be great.\",\n",
       "   'start': 138.09,\n",
       "   'end': 145.424,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah.',\n",
       "   'start': 152.131,\n",
       "   'end': 153.234,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" I don't know what this is.\",\n",
       "   'start': 154.015,\n",
       "   'end': 155.866,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' I thought you were preparing it here and then taking it out. That might be really comfortable.',\n",
       "   'start': 157.767,\n",
       "   'end': 163.574,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" It's really functional, I think, because you can put some drinks and something to eat. If you invite some friends over, then you can just invite them with something to eat or drink, like a movie night, something like this.\",\n",
       "   'start': 174.151,\n",
       "   'end': 188.616,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you', 'start': 193.998, 'end': 194.015, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Yeah.',\n",
       "   'start': 194.762,\n",
       "   'end': 195.475,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' But did you get any kind of advice from the creation or designer or this kind of things?',\n",
       "   'start': 199.312,\n",
       "   'end': 205.255,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Mhm.', 'start': 209.244, 'end': 209.38, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you', 'start': 209.431, 'end': 209.448, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you.', 'start': 209.465, 'end': 209.55, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" Yeah, sizes, colors, textures, it's not that easy.\",\n",
       "   'start': 221.808,\n",
       "   'end': 226.358,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" That's great, I think at night it would give a really cozy environment when you just turn it on.\",\n",
       "   'start': 233.761,\n",
       "   'end': 239.533,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' This stuff is really fabulous.',\n",
       "   'start': 249.211,\n",
       "   'end': 250.671,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' you.', 'start': 252.521, 'end': 252.708, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Oh, I see.',\n",
       "   'start': 253.964,\n",
       "   'end': 254.966,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" But, I mean, it's really nice.\",\n",
       "   'start': 256.087,\n",
       "   'end': 257.632,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' And what about the other decorations? I mean, I think everything is great here.',\n",
       "   'start': 259.822,\n",
       "   'end': 264.576,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah, of course. It makes the overall thing.',\n",
       "   'start': 275.102,\n",
       "   'end': 277.258,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Oh.', 'start': 284.864, 'end': 285.0, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you', 'start': 285.0, 'end': 285.085, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' you', 'start': 285.085, 'end': 285.102, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Hmm.', 'start': 285.102, 'end': 285.289, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' you.', 'start': 285.289, 'end': 285.323, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you.', 'start': 285.323, 'end': 285.357, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Thank you.',\n",
       "   'start': 285.357,\n",
       "   'end': 285.407,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' So.', 'start': 289.516, 'end': 290.008, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' with the guy in the other space.',\n",
       "   'start': 293.014,\n",
       "   'end': 294.864,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Of course.',\n",
       "   'start': 307.869,\n",
       "   'end': 308.43,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you', 'start': 308.43, 'end': 308.531, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' you', 'start': 314.049, 'end': 314.1, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': \" I think it's on your head.\",\n",
       "   'start': 314.287,\n",
       "   'end': 315.696,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' But, leave them behind.',\n",
       "   'start': 316.511,\n",
       "   'end': 318.107,\n",
       "   'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Mhm.', 'start': 318.277, 'end': 318.667, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': \" I think it has a whole vibe in itself. And also the story behind it, it's really cool.\",\n",
       "   'start': 328.599,\n",
       "   'end': 333.676,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" It's not only about the object, but the story behind it. I think that's great.\",\n",
       "   'start': 335.747,\n",
       "   'end': 339.856,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" Yeah, 100%. It's like a reminiscence of that really nice time that you spent there.\",\n",
       "   'start': 359.754,\n",
       "   'end': 365.374,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' you', 'start': 369.669, 'end': 369.737, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Oh okay.',\n",
       "   'start': 374.236,\n",
       "   'end': 375.085,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah.',\n",
       "   'start': 377.241,\n",
       "   'end': 377.988,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" Aha, okay, that's pretty particular.\",\n",
       "   'start': 380.365,\n",
       "   'end': 382.436,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Mm-hmm.',\n",
       "   'start': 387.699,\n",
       "   'end': 388.362,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Bye.', 'start': 391.995, 'end': 392.165, 'speaker': 'SPEAKER_01'},\n",
       "  {'text': ' Wow, okay.',\n",
       "   'start': 392.64,\n",
       "   'end': 393.557,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Wow, okay.',\n",
       "   'start': 419.805,\n",
       "   'end': 420.586,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Uh huh.',\n",
       "   'start': 431.672,\n",
       "   'end': 432.317,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah, of course.',\n",
       "   'start': 438.039,\n",
       "   'end': 439.38,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" But that's cool.\",\n",
       "   'start': 444.032,\n",
       "   'end': 445.424,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" Where else have you been? I mean, you have been to Gambia, Morocco. I mean, you've traveled so much.\",\n",
       "   'start': 445.747,\n",
       "   'end': 452.114,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" That's great.\",\n",
       "   'start': 458.752,\n",
       "   'end': 459.601,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Mhm.', 'start': 462.47, 'end': 462.844, 'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Okay.',\n",
       "   'start': 465.306,\n",
       "   'end': 466.036,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" I think it's really cool that you bring some object from your trips and then you put it in your home. And I think it's really cool that you can gather all these stories together. I think it really adds up to the whole decoration of the place as well.\",\n",
       "   'start': 473.727,\n",
       "   'end': 487.53,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Uh-huh, yeah.',\n",
       "   'start': 492.453,\n",
       "   'end': 493.574,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" It's really nice\",\n",
       "   'start': 494.542,\n",
       "   'end': 495.543,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Yeah, yeah, yeah, yeah, they are. Yeah.',\n",
       "   'start': 503.727,\n",
       "   'end': 505.9,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': ' Oh, I see.',\n",
       "   'start': 521.418,\n",
       "   'end': 522.555,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" But that's great, really. Thank you very much for showing me.\",\n",
       "   'start': 528.871,\n",
       "   'end': 531.537,\n",
       "   'speaker': 'SPEAKER_00'},\n",
       "  {'text': \" You're welcome.\",\n",
       "   'start': 534.491,\n",
       "   'end': 535.17,\n",
       "   'speaker': 'SPEAKER_00'}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
