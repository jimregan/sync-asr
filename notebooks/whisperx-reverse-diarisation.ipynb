{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.diarize import DiarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HF_DIR = Path.home() / \".huggingface\"\n",
    "HF_TOKEN = HF_DIR / \"token\"\n",
    "TOKEN = \"\"\n",
    "if HF_DIR.is_dir() and HF_TOKEN.exists():\n",
    "    with open(str(HF_TOKEN)) as hf_tok:\n",
    "        TOKEN = hf_tok.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_pipe = DiarizationPipeline(use_auth_token=TOKEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = Path(\"/home/joregan/hsi/audio\")\n",
    "EG = AUDIO_PATH / \"hsi_3_0715_227_001_inter-002.wav\"\n",
    "diar_res = diar_pipe(str(EG), num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, diar_seg in diar_res.iterrows():\n",
    "    print(diar_seg[\"start\"], diar_seg[\"end\"], diar_seg[\"speaker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is just to confirm that the output of `merge_chunks` is similar in terms of timestamps to the diarisation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from whisperx.vad import load_vad_model, merge_chunks\n",
    "from whisperx.audio import load_audio, SAMPLE_RATE\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L336\n",
    "default_vad_options = {\n",
    "    \"vad_onset\": 0.500,\n",
    "    \"vad_offset\": 0.363\n",
    "}\n",
    "\n",
    "audio = load_audio(str(EG))\n",
    "\n",
    "chunk_size = 30\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L186\n",
    "vad_model = load_vad_model(torch.device(DEVICE), use_auth_token=None, **default_vad_options)\n",
    "vad_segments = vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "vad_segments = merge_chunks(\n",
    "    vad_segments,\n",
    "    chunk_size,\n",
    "    onset=default_vad_options[\"vad_onset\"],\n",
    "    offset=default_vad_options[\"vad_offset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.008532423208191127,\n",
       " 'end': 27.329351535836178,\n",
       " 'segments': [(0.008532423208191127, 2.841296928327645),\n",
       "  (5.6058020477815695, 10.179180887372015),\n",
       "  (11.527303754266212, 12.022184300341298),\n",
       "  (23.523890784982935, 27.329351535836178)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarised_chunks(filename):\n",
    "    diar_res = diar_pipe(filename, num_speakers=2)\n",
    "    res = []\n",
    "    for idx, diar_seg in diar_res.iterrows():\n",
    "        res.append({\n",
    "            \"start\": diar_seg[\"start\"],\n",
    "            \"end\": diar_seg[\"end\"],\n",
    "            \"segments\": [(diar_seg[\"start\"], diar_seg[\"end\"])],\n",
    "            \"speaker\": diar_seg[\"speaker\"]\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def write_wave(filename, data):\n",
    "    data_denorm = data * 32768.0\n",
    "    data16 = data_denorm.astype(np.int16)\n",
    "    output = wave.open(filename, \"w\")\n",
    "    # pcm_s16le, single channel\n",
    "    output.setnchannels(1)\n",
    "    output.setsampwidth(2)\n",
    "    output.setframerate(16000)\n",
    "    output.writeframes(data16.tobytes())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my reference\n",
    "_FORMATS = \"\"\"\n",
    "hsi_N_NNNN_NNN_NNN-mic.wav\n",
    "hsi_N_NNNN_NNN_NNN-micN-NNN.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_main.wav\n",
    "\"\"\"\n",
    "def get_speaker_id(filename, detected_speaker):\n",
    "    detected_speaker = detected_speaker.replace(\"SPEAKER_\", \"\")\n",
    "    if \"inter\" in filename or \"mic2\" in filename:\n",
    "        part = \"inter\"\n",
    "    elif \"main\" in filename or \"mic1\" in filename:\n",
    "        part = \"main\"\n",
    "    elif filename.endswith(\"-mic.wav\"):\n",
    "        # one file\n",
    "        part = \"inter\"\n",
    "    pieces = filename.split(\"_\")\n",
    "    return f\"hsi_{pieces[1]}_{part}_{detected_speaker}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory(speaker_id, base_dir=\"/home/joregan/hsi_mfa\"):\n",
    "    base_path = Path(base_dir)\n",
    "    if not base_path.is_dir():\n",
    "        base_path.mkdir()\n",
    "    speaker_path = base_path / speaker_id\n",
    "    if not speaker_path.is_dir():\n",
    "        speaker_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from whisperx.types import TranscriptionResult\n",
    "from typing import List, Union\n",
    "import faster_whisper\n",
    "from whisperx.asr import find_numeral_symbol_tokens, SingleSegment\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L173\n",
    "def transcribe(\n",
    "    self, audio: Union[str, np.ndarray], batch_size=None, num_workers=0, language=None, task=None, chunk_size=30, print_progress = False, combined_progress=False\n",
    ") -> TranscriptionResult:\n",
    "    filename = audio\n",
    "    if isinstance(audio, str):\n",
    "        audio = load_audio(audio)\n",
    "\n",
    "    def data(audio, segments):\n",
    "        for seg in segments:\n",
    "            f1 = int(seg['start'] * SAMPLE_RATE)\n",
    "            f2 = int(seg['end'] * SAMPLE_RATE)\n",
    "            # print(f2-f1)\n",
    "            yield {'inputs': audio[f1:f2]}\n",
    "\n",
    "    # vad_segments = self.vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "    # vad_segments = merge_chunks(\n",
    "    #     vad_segments,\n",
    "    #     chunk_size,\n",
    "    #     onset=self._vad_params[\"vad_onset\"],\n",
    "    #     offset=self._vad_params[\"vad_offset\"],\n",
    "    # )\n",
    "    vad_segments = get_diarised_chunks(filename)\n",
    "    if self.tokenizer is None:\n",
    "        language = language or self.detect_language(audio)\n",
    "        task = task or \"transcribe\"\n",
    "        self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                            self.model.model.is_multilingual, task=task,\n",
    "                                                            language=language)\n",
    "    else:\n",
    "        language = language or self.tokenizer.language_code\n",
    "        task = task or self.tokenizer.task\n",
    "        if task != self.tokenizer.task or language != self.tokenizer.language_code:\n",
    "            self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                                self.model.model.is_multilingual, task=task,\n",
    "                                                                language=language)\n",
    "            \n",
    "    if self.suppress_numerals:\n",
    "        previous_suppress_tokens = self.options.suppress_tokens\n",
    "        numeral_symbol_tokens = find_numeral_symbol_tokens(self.tokenizer)\n",
    "        print(f\"Suppressing numeral and symbol tokens\")\n",
    "        new_suppressed_tokens = numeral_symbol_tokens + self.options.suppress_tokens\n",
    "        new_suppressed_tokens = list(set(new_suppressed_tokens))\n",
    "        self.options = self.options._replace(suppress_tokens=new_suppressed_tokens)\n",
    "\n",
    "    segments: List[SingleSegment] = []\n",
    "    batch_size = batch_size or self._batch_size\n",
    "    total_segments = len(vad_segments)\n",
    "    for idx, out in enumerate(self.__call__(data(audio, vad_segments), batch_size=batch_size, num_workers=num_workers)):\n",
    "        if print_progress:\n",
    "            base_progress = ((idx + 1) / total_segments) * 100\n",
    "            percent_complete = base_progress / 2 if combined_progress else base_progress\n",
    "            print(f\"Progress: {percent_complete:.2f}%...\")\n",
    "        text = out['text']\n",
    "        if batch_size in [0, 1, None]:\n",
    "            text = text[0]\n",
    "        # write_mfa()\n",
    "        segments.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"start\": round(vad_segments[idx]['start'], 3),\n",
    "                \"end\": round(vad_segments[idx]['end'], 3),\n",
    "                \"speaker\": vad_segments[idx]['speaker']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # revert the tokenizer if multilingual inference is enabled\n",
    "    if self.preset_language is None:\n",
    "        self.tokenizer = None\n",
    "\n",
    "    # revert suppressed tokens if suppress_numerals is enabled\n",
    "    if self.suppress_numerals:\n",
    "        self.options = self.options._replace(suppress_tokens=previous_suppress_tokens)\n",
    "\n",
    "    return {\"segments\": segments, \"language\": language}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "\n",
    "- [X] write MFA files\n",
    "  - [X] write wav\n",
    "  - [X] normalise text\n",
    "  - [X] write text\n",
    "  - [X] write offsets against original file\n",
    "- [X] write basic output\n",
    "- [ ] pass prompt\n",
    "- [ ] enforce language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import types\n",
    "\n",
    "compute_type = \"float16\"\n",
    "batch_size = 16\n",
    "model = whisperx.load_model(\"large-v2\", DEVICE, compute_type=compute_type)\n",
    "model.transcribe = types.MethodType(transcribe, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text + \" \"\n",
    "    # https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L320C1-L321C53\n",
    "    prepend_punctuations = \"\\\"'“¿([{-\"\n",
    "    append_punctuations = \"\\\"'.。,，!！?？:：”)]}、\"\n",
    "    text = text.replace(\"...\", \"\")\n",
    "    for punct in prepend_punctuations:\n",
    "        text = text.replace(f\" {punct}\", \" \")\n",
    "    for punct in append_punctuations:\n",
    "        text = text.replace(f\"{punct} \", \" \")\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mfa(filename, audio, segment, base_path=\"/home/joregan/hsi_mfa\"):\n",
    "    seg_id = get_speaker_id(filename, segment['speaker'])\n",
    "    ensure_directory(seg_id, base_path)\n",
    "    filestem = Path(filename).stem\n",
    "    output_base = f\"{filestem}__{segment['start']}_{segment['end']}\"\n",
    "    f1 = int(segment['start'] * SAMPLE_RATE)\n",
    "    f2 = int(segment['end'] * SAMPLE_RATE)\n",
    "    audio_segment = audio[f1:f2]\n",
    "    clean = clean_text(segment['text'])\n",
    "    base_path_path = Path(base_path)\n",
    "    text_filename = base_path_path / f\"{output_base}.txt\"\n",
    "    with open(text_filename, \"w\") as txtf:\n",
    "        txtf.write(clean)\n",
    "    wave_filename = base_path_path / f\"{output_base}.wav\"\n",
    "    write_wave(wave_filename, audio_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en (0.98) in first 30s of audio...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tsvf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mend\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mspeaker_id\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegments\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mwrite_mfa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     tsvf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msegment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msegment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msegment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeaker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msegment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[62], line 15\u001b[0m, in \u001b[0;36mwrite_mfa\u001b[0;34m(filename, audio, segment, base_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     txtf\u001b[38;5;241m.\u001b[39mwrite(clean)\n\u001b[1;32m     14\u001b[0m wave_filename \u001b[38;5;241m=\u001b[39m base_path_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mwrite_wave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwave_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_segment\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 12\u001b[0m, in \u001b[0;36mwrite_wave\u001b[0;34m(filename, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m output\u001b[38;5;241m.\u001b[39msetsampwidth(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m output\u001b[38;5;241m.\u001b[39msetframerate(\u001b[38;5;241m16000\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m output\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/whisperx/lib/python3.10/wave.py:437\u001b[0m, in \u001b[0;36mWave_write.writeframes\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriteframes\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteframesraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datalength \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datawritten:\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patchheader()\n",
      "File \u001b[0;32m~/miniconda3/envs/whisperx/lib/python3.10/wave.py:426\u001b[0m, in \u001b[0;36mWave_write.writeframesraw\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m    425\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(data)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_header_written\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m nframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampwidth \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nchannels)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisperx/lib/python3.10/wave.py:467\u001b[0m, in \u001b[0;36mWave_write._ensure_header_written\u001b[0;34m(self, datasize)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_framerate:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling rate not specified\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 467\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisperx/lib/python3.10/wave.py:471\u001b[0m, in \u001b[0;36mWave_write._write_header\u001b[0;34m(self, initlength)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_header\u001b[39m(\u001b[38;5;28mself\u001b[39m, initlength):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headerwritten\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nframes:\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nframes \u001b[38;5;241m=\u001b[39m initlength \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nchannels \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampwidth)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "filename = str(EG)\n",
    "result = model.transcribe(audio=filename, batch_size=batch_size)\n",
    "full_audio = load_audio(filename)\n",
    "\n",
    "tsv_file = filename.replace(\".wav\", \"_segments.tsv\")\n",
    "with open(tsv_file, \"w\") as tsvf:\n",
    "    tsvf.write(\"filename\\tstart\\tend\\tspeaker_id\\ttext\\n\")\n",
    "    for segment in result['segments']:\n",
    "        write_mfa(filename, full_audio, segment)\n",
    "        tsvf.write(f\"{filename}\\t{segment['start']}\\t{segment['end']}\\t{segment['speaker']}\\t{segment['text'].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
