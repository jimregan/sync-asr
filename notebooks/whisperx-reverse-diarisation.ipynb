{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.diarize import DiarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HF_DIR = Path.home() / \".huggingface\"\n",
    "HF_TOKEN = HF_DIR / \"token\"\n",
    "TOKEN = \"\"\n",
    "if HF_DIR.is_dir() and HF_TOKEN.exists():\n",
    "    with open(str(HF_TOKEN)) as hf_tok:\n",
    "        TOKEN = hf_tok.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_pipe = DiarizationPipeline(use_auth_token=TOKEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = Path(\"/home/joregan/hsi/audio\")\n",
    "EG = AUDIO_PATH / \"hsi_3_0715_227_001_inter-002.wav\"\n",
    "diar_res = diar_pipe(str(EG), num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, diar_seg in diar_res.iterrows():\n",
    "    print(diar_seg[\"start\"], diar_seg[\"end\"], diar_seg[\"speaker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is just to confirm that the output of `merge_chunks` is similar in terms of timestamps to the diarisation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from whisperx.vad import load_vad_model, merge_chunks\n",
    "from whisperx.audio import load_audio, SAMPLE_RATE\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L336\n",
    "default_vad_options = {\n",
    "    \"vad_onset\": 0.500,\n",
    "    \"vad_offset\": 0.363\n",
    "}\n",
    "\n",
    "audio = load_audio(str(EG))\n",
    "\n",
    "chunk_size = 30\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L186\n",
    "vad_model = load_vad_model(torch.device(DEVICE), use_auth_token=None, **default_vad_options)\n",
    "vad_segments = vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "vad_segments = merge_chunks(\n",
    "    vad_segments,\n",
    "    chunk_size,\n",
    "    onset=default_vad_options[\"vad_onset\"],\n",
    "    offset=default_vad_options[\"vad_offset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.008532423208191127,\n",
       " 'end': 27.329351535836178,\n",
       " 'segments': [(0.008532423208191127, 2.841296928327645),\n",
       "  (5.6058020477815695, 10.179180887372015),\n",
       "  (11.527303754266212, 12.022184300341298),\n",
       "  (23.523890784982935, 27.329351535836178)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarised_chunks(filename):\n",
    "    diar_res = diar_pipe(filename, num_speakers=2)\n",
    "    res = []\n",
    "    for idx, diar_seg in diar_res.iterrows():\n",
    "        res.append({\n",
    "            \"start\": diar_seg[\"start\"],\n",
    "            \"end\": diar_seg[\"end\"],\n",
    "            \"segments\": [(diar_seg[\"start\"], diar_seg[\"end\"])],\n",
    "            \"speaker\": diar_seg[\"speaker\"]\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def write_wave(filename, data):\n",
    "    data_denorm = data * 32768.0\n",
    "    data16 = data_denorm.astype(np.int16)\n",
    "    output = wave.open(filename, \"w\")\n",
    "    # pcm_s16le, single channel\n",
    "    output.setnchannels(1)\n",
    "    output.setsampwidth(2)\n",
    "    output.setframerate(16000)\n",
    "    output.writeframes(data16.tobytes())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my reference\n",
    "_FORMATS = \"\"\"\n",
    "hsi_N_NNNN_NNN_NNN-mic.wav\n",
    "hsi_N_NNNN_NNN_NNN-micN-NNN.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_main.wav\n",
    "\"\"\"\n",
    "def get_speaker_id(filename, detected_speaker):\n",
    "    detected_speaker = detected_speaker.replace(\"SPEAKER_\", \"\")\n",
    "    if \"inter\" in filename or \"mic2\" in filename:\n",
    "        part = \"inter\"\n",
    "    elif \"main\" in filename or \"mic1\" in filename:\n",
    "        part = \"main\"\n",
    "    elif filename.endswith(\"-mic.wav\"):\n",
    "        # one file\n",
    "        part = \"inter\"\n",
    "    pieces = filename.split(\"_\")\n",
    "    return f\"hsi_{pieces[1]}_{part}_{detected_speaker}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory(speaker_id, base_dir=\"/home/joregan/hsi_mfa\"):\n",
    "    base_path = Path(base_dir)\n",
    "    if not base_path.is_dir():\n",
    "        base_path.mkdir()\n",
    "    speaker_path = base_path / speaker_id\n",
    "    if not speaker_path.is_dir():\n",
    "        speaker_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from whisperx.types import TranscriptionResult\n",
    "from typing import List, Union\n",
    "import faster_whisper\n",
    "from whisperx.asr import find_numeral_symbol_tokens, SingleSegment\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L173\n",
    "def transcribe(\n",
    "    self, audio: Union[str, np.ndarray], batch_size=None, num_workers=0, language=None, task=None, chunk_size=30, print_progress = False, combined_progress=False\n",
    ") -> TranscriptionResult:\n",
    "    filename = audio\n",
    "    if isinstance(audio, str):\n",
    "        audio = load_audio(audio)\n",
    "\n",
    "    def data(audio, segments):\n",
    "        for seg in segments:\n",
    "            f1 = int(seg['start'] * SAMPLE_RATE)\n",
    "            f2 = int(seg['end'] * SAMPLE_RATE)\n",
    "            # print(f2-f1)\n",
    "            yield {'inputs': audio[f1:f2]}\n",
    "\n",
    "    # vad_segments = self.vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "    # vad_segments = merge_chunks(\n",
    "    #     vad_segments,\n",
    "    #     chunk_size,\n",
    "    #     onset=self._vad_params[\"vad_onset\"],\n",
    "    #     offset=self._vad_params[\"vad_offset\"],\n",
    "    # )\n",
    "    vad_segments = get_diarised_chunks(filename)\n",
    "    if self.tokenizer is None:\n",
    "        language = language or self.detect_language(audio)\n",
    "        task = task or \"transcribe\"\n",
    "        self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                            self.model.model.is_multilingual, task=task,\n",
    "                                                            language=language)\n",
    "    else:\n",
    "        language = language or self.tokenizer.language_code\n",
    "        task = task or self.tokenizer.task\n",
    "        if task != self.tokenizer.task or language != self.tokenizer.language_code:\n",
    "            self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                                self.model.model.is_multilingual, task=task,\n",
    "                                                                language=language)\n",
    "            \n",
    "    if self.suppress_numerals:\n",
    "        previous_suppress_tokens = self.options.suppress_tokens\n",
    "        numeral_symbol_tokens = find_numeral_symbol_tokens(self.tokenizer)\n",
    "        print(f\"Suppressing numeral and symbol tokens\")\n",
    "        new_suppressed_tokens = numeral_symbol_tokens + self.options.suppress_tokens\n",
    "        new_suppressed_tokens = list(set(new_suppressed_tokens))\n",
    "        self.options = self.options._replace(suppress_tokens=new_suppressed_tokens)\n",
    "\n",
    "    segments: List[SingleSegment] = []\n",
    "    batch_size = batch_size or self._batch_size\n",
    "    total_segments = len(vad_segments)\n",
    "    for idx, out in enumerate(self.__call__(data(audio, vad_segments), batch_size=batch_size, num_workers=num_workers)):\n",
    "        if print_progress:\n",
    "            base_progress = ((idx + 1) / total_segments) * 100\n",
    "            percent_complete = base_progress / 2 if combined_progress else base_progress\n",
    "            print(f\"Progress: {percent_complete:.2f}%...\")\n",
    "        text = out['text']\n",
    "        if batch_size in [0, 1, None]:\n",
    "            text = text[0]\n",
    "        # write_mfa()\n",
    "        segments.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"start\": round(vad_segments[idx]['start'], 3),\n",
    "                \"end\": round(vad_segments[idx]['end'], 3),\n",
    "                \"speaker\": vad_segments[idx]['speaker']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # revert the tokenizer if multilingual inference is enabled\n",
    "    if self.preset_language is None:\n",
    "        self.tokenizer = None\n",
    "\n",
    "    # revert suppressed tokens if suppress_numerals is enabled\n",
    "    if self.suppress_numerals:\n",
    "        self.options = self.options._replace(suppress_tokens=previous_suppress_tokens)\n",
    "\n",
    "    return {\"segments\": segments, \"language\": language}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "\n",
    "- [ ] write MFA files\n",
    "  - [X] write wav\n",
    "  - [ ] normalise text\n",
    "  - [ ] write text\n",
    "  - [ ] write offsets against original file\n",
    "- [ ] write basic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import types\n",
    "\n",
    "compute_type = \"float16\"\n",
    "batch_size = 16\n",
    "model = whisperx.load_model(\"large-v2\", DEVICE, compute_type=compute_type)\n",
    "model.transcribe = types.MethodType(transcribe, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text + \" \"\n",
    "    # https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L320C1-L321C53\n",
    "    prepend_punctuations = \"\\\"'“¿([{-\"\n",
    "    append_punctuations = \"\\\"'.。,，!！?？:：”)]}、\"\n",
    "    text = text.replace(\"...\", \"\")\n",
    "    for punct in prepend_punctuations:\n",
    "        text = text.replace(f\" {punct}\", \" \")\n",
    "    for punct in append_punctuations:\n",
    "        text = text.replace(f\"{punct} \", \" \")\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mfa(filename, audio, segment, base_path=\"/home/joregan/hsi_mfa\"):\n",
    "    seg_id = get_speaker_id(filename, segment['speaker'])\n",
    "    ensure_directory(seg_id, base_path)\n",
    "    filestem = Path(filename).stem\n",
    "    output_base = f\"{filestem}__{segment['start']}_{segment['end']}\"\n",
    "    f1 = int(segment['start'] * SAMPLE_RATE)\n",
    "    f2 = int(segment['end'] * SAMPLE_RATE)\n",
    "    audio_segment = audio[f1:f2]\n",
    "    clean = clean_text(segment['text'])\n",
    "    base_path_path = Path(base_path)\n",
    "    text_filename = base_path_path / f\"{output_base}.txt\"\n",
    "    with open(text_filename, \"w\") as txtf:\n",
    "        txtf.write(clean)\n",
    "    wave_filename = base_path_path / f\"{output_base}.wav\"\n",
    "    write_wave(wave_filename, audio_segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = str(EG)\n",
    "result = model.transcribe(audio=filename, batch_size=batch_size)\n",
    "full_audio = load_audio(filename)\n",
    "\n",
    "tsv_file = filename.replace(\".wav\", \"_segments.tsv\")\n",
    "with open(tsv_file, \"w\") as tsvf:\n",
    "    tsvf.write(\"filename\\tstart\\tend\\tspeaker_id\\ttext\\n\")\n",
    "    for segment in result['segments']:\n",
    "        write_mfa(filename, full_audio, segment)\n",
    "        tsvf.write(f\"{filename}\\t{segment['start']}\\t{segment['end']}\\t{segment['speaker']}\\t{segment['text'].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
