{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.diarize import DiarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HF_DIR = Path.home() / \".huggingface\"\n",
    "HF_TOKEN = HF_DIR / \"token\"\n",
    "TOKEN = \"\"\n",
    "if HF_DIR.is_dir() and HF_TOKEN.exists():\n",
    "    with open(str(HF_TOKEN)) as hf_tok:\n",
    "        TOKEN = hf_tok.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_pipe = DiarizationPipeline(use_auth_token=TOKEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = Path(\"/home/joregan/hsi/audio\")\n",
    "EG = AUDIO_PATH / \"hsi_3_0715_227_001_inter-002.wav\"\n",
    "diar_res = diar_pipe(str(EG), num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, diar_seg in diar_res.iterrows():\n",
    "    print(diar_seg[\"start\"], diar_seg[\"end\"], diar_seg[\"speaker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is just to confirm that the output of `merge_chunks` is similar in terms of timestamps to the diarisation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from whisperx.vad import load_vad_model, merge_chunks\n",
    "from whisperx.audio import load_audio, SAMPLE_RATE\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L336\n",
    "default_vad_options = {\n",
    "    \"vad_onset\": 0.500,\n",
    "    \"vad_offset\": 0.363\n",
    "}\n",
    "\n",
    "audio = load_audio(str(EG))\n",
    "\n",
    "chunk_size = 30\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L186\n",
    "vad_model = load_vad_model(torch.device(DEVICE), use_auth_token=None, **default_vad_options)\n",
    "vad_segments = vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "vad_segments = merge_chunks(\n",
    "    vad_segments,\n",
    "    chunk_size,\n",
    "    onset=default_vad_options[\"vad_onset\"],\n",
    "    offset=default_vad_options[\"vad_offset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.008532423208191127,\n",
       " 'end': 27.329351535836178,\n",
       " 'segments': [(0.008532423208191127, 2.841296928327645),\n",
       "  (5.6058020477815695, 10.179180887372015),\n",
       "  (11.527303754266212, 12.022184300341298),\n",
       "  (23.523890784982935, 27.329351535836178)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarised_chunks(filename):\n",
    "    diar_res = diar_pipe(filename, num_speakers=2)\n",
    "    res = []\n",
    "    for idx, diar_seg in diar_res.iterrows():\n",
    "        res.append({\n",
    "            \"start\": diar_seg[\"start\"],\n",
    "            \"end\": diar_seg[\"end\"],\n",
    "            \"segments\": [(diar_seg[\"start\"], diar_seg[\"end\"])],\n",
    "            \"speaker\": diar_seg[\"speaker\"]\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def write_wave(filename, data):\n",
    "    data_denorm = data * 32768.0\n",
    "    data16 = data_denorm.astype(np.int16)\n",
    "    output = wave.open(filename, \"w\")\n",
    "    # pcm_s16le, single channel\n",
    "    output.setnchannels(1)\n",
    "    output.setsampwidth(2)\n",
    "    output.setframerate(16000)\n",
    "    output.writeframes(data16.tobytes())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from whisperx.types import TranscriptionResult\n",
    "from typing import List, Union\n",
    "import faster_whisper\n",
    "from whisperx.asr import find_numeral_symbol_tokens, SingleSegment\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L173\n",
    "def transcribe(\n",
    "    self, audio: Union[str, np.ndarray], batch_size=None, num_workers=0, language=None, task=None, chunk_size=30, print_progress = False, combined_progress=False\n",
    ") -> TranscriptionResult:\n",
    "    filename = audio\n",
    "    if isinstance(audio, str):\n",
    "        audio = load_audio(audio)\n",
    "\n",
    "    def data(audio, segments):\n",
    "        for seg in segments:\n",
    "            f1 = int(seg['start'] * SAMPLE_RATE)\n",
    "            f2 = int(seg['end'] * SAMPLE_RATE)\n",
    "            # print(f2-f1)\n",
    "            yield {'inputs': audio[f1:f2]}\n",
    "\n",
    "    # vad_segments = self.vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "    # vad_segments = merge_chunks(\n",
    "    #     vad_segments,\n",
    "    #     chunk_size,\n",
    "    #     onset=self._vad_params[\"vad_onset\"],\n",
    "    #     offset=self._vad_params[\"vad_offset\"],\n",
    "    # )\n",
    "    vad_segments = get_diarised_chunks(filename)\n",
    "    if self.tokenizer is None:\n",
    "        language = language or self.detect_language(audio)\n",
    "        task = task or \"transcribe\"\n",
    "        self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                            self.model.model.is_multilingual, task=task,\n",
    "                                                            language=language)\n",
    "    else:\n",
    "        language = language or self.tokenizer.language_code\n",
    "        task = task or self.tokenizer.task\n",
    "        if task != self.tokenizer.task or language != self.tokenizer.language_code:\n",
    "            self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                                self.model.model.is_multilingual, task=task,\n",
    "                                                                language=language)\n",
    "            \n",
    "    if self.suppress_numerals:\n",
    "        previous_suppress_tokens = self.options.suppress_tokens\n",
    "        numeral_symbol_tokens = find_numeral_symbol_tokens(self.tokenizer)\n",
    "        print(f\"Suppressing numeral and symbol tokens\")\n",
    "        new_suppressed_tokens = numeral_symbol_tokens + self.options.suppress_tokens\n",
    "        new_suppressed_tokens = list(set(new_suppressed_tokens))\n",
    "        self.options = self.options._replace(suppress_tokens=new_suppressed_tokens)\n",
    "\n",
    "    segments: List[SingleSegment] = []\n",
    "    batch_size = batch_size or self._batch_size\n",
    "    total_segments = len(vad_segments)\n",
    "    for idx, out in enumerate(self.__call__(data(audio, vad_segments), batch_size=batch_size, num_workers=num_workers)):\n",
    "        if print_progress:\n",
    "            base_progress = ((idx + 1) / total_segments) * 100\n",
    "            percent_complete = base_progress / 2 if combined_progress else base_progress\n",
    "            print(f\"Progress: {percent_complete:.2f}%...\")\n",
    "        text = out['text']\n",
    "        if batch_size in [0, 1, None]:\n",
    "            text = text[0]\n",
    "        segments.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"start\": round(vad_segments[idx]['start'], 3),\n",
    "                \"end\": round(vad_segments[idx]['end'], 3),\n",
    "                \"speaker\": vad_segments[idx]['speaker']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # revert the tokenizer if multilingual inference is enabled\n",
    "    if self.preset_language is None:\n",
    "        self.tokenizer = None\n",
    "\n",
    "    # revert suppressed tokens if suppress_numerals is enabled\n",
    "    if self.suppress_numerals:\n",
    "        self.options = self.options._replace(suppress_tokens=previous_suppress_tokens)\n",
    "\n",
    "    return {\"segments\": segments, \"language\": language}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
